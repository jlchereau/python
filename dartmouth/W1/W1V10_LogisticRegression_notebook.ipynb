{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we will use the package scikit-learn to perform logistic regression. As with linear regression, the first step is to load the data we will use into a format suitable for training a logistic regression model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We again read external files into a `DataFrame`, loading the readmission data from an earlier lesson from this week."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load and view data\n",
    "import io\n",
    "import pandas as pd\n",
    "readmission_data = pd.read_csv('logreg_readmission_multivariate.csv')\n",
    "readmission_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Univariate Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just as with linear regression, the code below separates the data into features x and outcomes y, which in this case is readmission. Next part of the code imports the `LogisticRegression` class, and fits the model. The `penalty` parameter is set to `none`. Other values of this parameter would modify how the model is trained. We will discuss this in a lesson next week. Finally, we extract the coefficient values as before and print them for viewing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# seperate data into features and outcomes\n",
    "X_train = readmission_data[['Length of Stay']]\n",
    "y_train = readmission_data.Readmission\n",
    "\n",
    "#train create and train model\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "logistic_reg_model1 = LogisticRegression(penalty='none')\n",
    "logistic_reg_model1.fit(X_train, y_train)\n",
    "   \n",
    "#extract coefficients\n",
    "theta0 = logistic_reg_model1.intercept_[0]\n",
    "theta1 = logistic_reg_model1.coef_[0][0]\n",
    "print(\"fitted theta1=%s, fitted theta0=%s\" % (theta1, theta0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use these coefficients to plot the logistic curve showing probability of readmission against length of stay. Note that this time, the variable `new_y` that gives the fitted curve is computed by the logistic expression, `1/(1+np.exp(-theta0-theta1*new_x))`. We have passed the `scatter` function the additional parameter `alpha`, whichallows the blue data points to be darkened according to how many data points share that `Length of Stay` value. We can see that the left side of the plot has many dark blue points associated with no readmission, and the right side of the plot – corresponding to a longer length of stay – is associated more with readmission."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load plotting library and numerical library\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "#plot raw data\n",
    "plt.scatter(x=readmission_data['Length of Stay'], y=readmission_data['Readmission'],alpha=.03)\n",
    "plt.title('Readmission vs Length of Stay')\n",
    "plt.xlabel('Length of Stay')\n",
    "plt.ylabel('Probability of Readmission')\n",
    "\n",
    "#plot fitted logistic curve\n",
    "new_x = np.arange(start=0,stop=22,step=0.1)\n",
    "new_y = 1/(1+np.exp(-theta0-theta1*new_x))\n",
    "plt.plot(new_x,new_y,color=\"red\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just as with linear regression, we can get predictions for a given set of features using the `.predict()` method. For logistic regression, this gives a value of 1 for datapoints with a predicted probability of 0.5 or higher, and a value of 0 for datapoints with a predicted probability less than that. To get the raw probabilities, we can use the method `.predict_proba()` which returns two columns, which are the predicted probabilities for 0 and 1, respectively. We use this method below to predict the readmission probabilities for three new lengths of stay."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create new Data Frame with new Lengths of Stay\n",
    "readmission_prediction_df = pd.DataFrame({\"Length of Stay\":[1.0,7.0,14.0]})\n",
    "#make predictions of readmission probability (returns column for 0 and 1)\n",
    "predictions = logistic_reg_model1.predict_proba(readmission_prediction_df[['Length of Stay']])\n",
    "#add predictions to Data Frame \n",
    "readmission_prediction_df['Readmission Probability'] = predictions[:,1]\n",
    "print(readmission_prediction_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we consider predicted probabilities of over 0.5 to be a predicted readmission, we can look at true and false classifications using a confusion matrix. This matrix shows the number of datapoints for which correct and incorrect predictions were made, for either a true value of 0 or a true value of 1. This is a useful way of evaluating the fit of a logistic model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Visualize predictions vs. true values \n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "ConfusionMatrixDisplay.from_estimator(logistic_reg_model1, X_train, y_train)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multivariate Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic regression with more than one feature proceeds very similarly. We simply specify the additional features we want in our training data – in this case they are age, gender, and number of medical conditions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# seperate data into features and outcomes\n",
    "X_train = readmission_data[['Length of Stay', 'Age', 'Gender (Female=1)',\n",
    "       'No. of Medical Conditions']]\n",
    "y_train = readmission_data.Readmission\n",
    "#train create and train model\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "logistic_reg_model2 = LogisticRegression(penalty='none')\n",
    "logistic_reg_model2.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As before, we can visualize model performance by looking at a confusion matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Visualize predictions vs. true values\n",
    "ConfusionMatrixDisplay.from_estimator(logistic_reg_model2, X_train, y_train)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the number of cells where the true and predicted labels do not match is lower than in the univariate case, indicating an improved fit."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
